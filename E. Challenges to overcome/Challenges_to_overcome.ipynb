{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Challenges to overcome\n"
      ],
      "metadata": {
        "id": "j64rBTsSBwpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computational time\n"
      ],
      "metadata": {
        "id": "TEOiGnFFBzQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My model consistently took a significant amount of time to fit. It took around 10 seconds for each epoch. This is surprising due to the fact that my dataset is small (however I did use data augmentation to give the model more things to train on). I also used cacheing and prefetching in an effort to make loading images faster. Even tho the training speed is moderately slow, it is definetly still in reasonable limits and I did what I could to reduce the time it takes. Therfore, I am not too worried about it."
      ],
      "metadata": {
        "id": "fqZhUHu-CjXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overfitting\n"
      ],
      "metadata": {
        "id": "BhTGcXuDB5a9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially my model had high training accuracy and low validation accuracy, this indicated overfitting. Essentally, my model was memorizing the training examples instead of learning patterns. The model was also fluctuating accuracy values significantly indicating the same problem.\n",
        "\n",
        "I already had two methods already in place to combat potential overfitting, these were:\n",
        "1. Data Augmentation - my introducing variations in images I exposed my model to a wider range of data\n",
        "2. Normilization - by normalizing pixel values I improved the stablility and efficency which contributed to smoother learning.\n",
        "\n",
        "In order to combat the overfitting I also added:\n",
        "1. L2 Regularization - this penalized large weights and encouraged the model to learn simpler patterns\n",
        "2. Dropout - randomly dropped neurons during training, preventing over-reliance\n",
        "3. Early Stopping - prevents the model from training for too long by stopping when the validation accuracy plateaus or starts to decrease\n",
        "\n",
        "As a result, I succeeded in imporving my validation accuracy, reducing overfitting and having more stabilized learning."
      ],
      "metadata": {
        "id": "YWr7_a4NEBnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Low Performance"
      ],
      "metadata": {
        "id": "EU8_B-PtB-YT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying L2 regularization and dropout I noticed a decrease in model performance/accuracy. To fix this I addressed the values used and lowered the regularizer from `0.01` to `0.001` and decreased the dropout of the dense layers from `0.5` to `0.25` and the dropout of conv layers from `0.25` to `0.15`. This successfully improved my model."
      ],
      "metadata": {
        "id": "Ew-p7w8NFcj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Undesirable Evaluation Metrics"
      ],
      "metadata": {
        "id": "PPlEdDvqCG2e"
      }
    }
  ]
}